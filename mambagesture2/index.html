<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Generating human gesture using multi-modality diffusion model">
  <meta name="keywords" content="montion generation, diffusion, multi-modality">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MambaGesture2: Co-Speech Gesture Generation via Hierarchical Fusion and Spatiotemporal Aggregation</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-BNGC9CJK2B"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-BNGC9CJK2B');
  </script>

</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://fcchit.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://fcchit.github.io/mambagesture/">
            MambaGesture
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">MambaGesture2: Co-Speech Gesture Generation via Hierarchical Fusion and Spatiotemporal Aggregation</h1>
          <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=xiK4nFUAAAAJ">Yabiao Wang</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=s61j748AAAAJ">Chencan Fu</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://zhangzjn.github.io/">Jiangning Zhang</a>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=8NfQv1sAAAAJ">Haoyang he</a>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=6toGO3wAAAAJ">Shuo Wang</a>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=fqte5H4AAAAJ">Chengjie Wang</a>,</span>
            <span class="author-block">
              <a href="https://tyshiwo.github.io/">Ying Tai</a>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=qYcgBbEAAAAJ">Yong Liu</a><sup>+</sup>,</span>
          </div>

          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Zhejiang University,</span>
            <span class="author-block"><sup>2</sup>Tencent,</span>
            <span class="author-block"><sup>3</sup>Fudan University,</span>
            <span class="author-block"><sup>4</sup>Shanghai Jiao Tong University,</span>
            <span class="author-block"><sup>5</sup>VIVO</span>
          </div> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
      <!-- Demo Video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Demo Video</h2>
          <div class="content">
            <p>
              Watch our demo video to see MambaGesture2 in action!
            </p>
            <div class="video-container">
              <iframe width="560" height="315" src="https://www.youtube.com/embed/YOUR_VIDEO_ID" frameborder="0" allowfullscreen></iframe>
              <!-- Replace YOUR_VIDEO_ID with the actual ID of your YouTube video -->
            </div>
          </div>
        </div>
      </div>
      <!--/ Demo Video. -->
    </div>
  </section>
  
  <style>
  .video-container {
    position: relative;
    padding-bottom: 56.25%; /* 16:9 Aspect Ratio */
    height: 0;
    overflow: hidden;
  }
  .video-container iframe {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
  }
  </style>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Co-speech gesture generation is crucial for producing synchronized and realistic human gestures, thereby enhancing the animation of lifelike avatars in virtual environments. While diffusion models have demonstrated remarkable capabilities, their combination with the quadratic complexity of transformers leads to increased resource consumption. Furthermore, as a time series task, current models often struggle to effectively capture multi-scale temporal dynamics. To address these challenges, we introduce MambaGesture2, a novel framework that integrates a Mamba-based network, <strong>Hierarchical U-Net Gesture Mamba (HUG-Mamba)</strong>, with a multi-modality feature fusion module, <strong>SEAD</strong>. HUG-Mamba leverages the sequential data processing strengths of the Mamba model alongside the U-Net architecture, significantly enhancing the temporal coherence of generated gestures. Specifically, we propose the <strong>Temporal-Stratified Fusion (TSF)</strong> module, which employs multi-scale learning to capture diverse temporal information. Additionally, we design the <strong>Spatial-Temporal Cascaded Aggregation (STCA)</strong> module to improve both spatial and temporal modeling. Our approach is rigorously evaluated on the multi-modal BEAT2 and SHOW datasets, demonstrating significant improvements across comprehensive metrics and achieving state-of-the-art performance in co-speech gesture generation. For more information, please visit the project link: <a href="https://fcchit.github.io/mambagesture2" target="_blank">https://fcchit.github.io/mambagesture2</a>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/fcchit" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
